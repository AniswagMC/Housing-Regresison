---
title: "Stage 3 report"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(ggcorrplot)
library(corrplot)
library(tidyverse)
library("GGally")
library(patchwork)
library(dplyr)
library("randomForest")
songs <- read.csv("joined_billboard_audiofeature.csv")
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)

train_raw <- read.csv("training.csv")
train_raw <- train_raw  %>% dplyr::select(-c("X"))

n <- nrow(train_raw)
trainidx <- sample.int(n, floor(n * .75))
validationidx <- setdiff(1:n, trainidx)
train <- train_raw[trainidx, ]
validation <- train_raw[validationidx,]

test <- read.csv("test.csv")
```


## Introduction
The best example to illustrate the commercialization of the arts is today’s music industry. Being a multi-billion dollar industry, artists and labels have to work on finding a balance between making music that appeals to their fans and fulfills their artistic expression, while also makes money. In the digital landscape, the number of plays or streams a song gets is indicative of it’s financial success. In other words, the more popular a song becomes, the more money it brings in. If this really is the case, then being able to idenfity how popular a song can become before it is even released would be of great interest to labels and record companies. So, this naturally raises the question that we will be focusing on in this report: 

**Given data on attributes of a song, can we predict how popular it will become?**

### Dataset
To answer this question, our group will be working with data on different audio-based features of a song to predict it's popularity when released on a music streaming platform. The dataset we will be using is from the Tidy Tuesday dataset collection [https://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-09-14/readme.md]. We have cleaned and combined the `audio_features.csv` and `billboard.csv` datasets provided to obtain the dataset we will use for the rest of our analysis. The raw data files have 32 variables, of which we will be using 21 relevant variables, removing the variables that don't affect our dataset, such as URLs and id's for the songs. We also narrowed the billboard data by selecting only the rows which a song peaks on the chart, and ignoring other weeks, then removing the week specific variables. The full transformation can be found in the `Data_Transformation_script_Billboard.R` script. The remaining data represents different features of a song such as its key, tempo, loudness, danceability, etc. as well as metrics reflecting its popularity on billboard and spotify charts respectively. 

### Objective
Our goal with this project/report is to create a regression model to predict how "popular" a song may be based on it's audio-based features. This means we will be looking at each song independent of cultural trends, the associated artist's popularity, and other external influences. We will be looking into ideas such as what features popular songs have in common, if any specific feature is highly linked to a song performing well or being well recieved, and whether we can identify any subgroups with their own unique trends.

### Motivation 
We as a group agreed to working with this dataset and on this question because we are all avid music lovers. With varied interest and tastes amongst all of us, we thought it would be interesting to see how an emperical analysis of what is or isn't "popular music" would compare to our own personal preferences, and how much each of us may agree or disagree with the findings.

## Exploratory Data Analysis

### Quick briefing of selection of response variable

\vspace{10pt}

We first cleaned our dataset by removing obviously non-useful variables like names. (However, while we recognize that different genres may have differing levels of popularity, for simplicity we will leave out this variable as the "genre" of certain songs may be ambiguous and there are too many categories to keep track of.)
```{r, echo=FALSE}
bd_clean <- dplyr::select(songs, -performer, -song, -spotify_genre, -spotify_track_album, -date)
```

Next we make a correlation plot to have a broad overview of what variables may be connected to our response as well as with each other:
```{r, echo = FALSE, fig.align = 'center', fig.height=4}
options(repr.plot.width=10, repr.plot.height=10)
cor_matrix <- round(cor(bd_clean), 1)
corrplot(cor_matrix,
   method = "color", 
   addCoef.col="grey", 
   order = "AOE", 
   number.cex=0.4)
```

We had briefly explained in our stage 2 report that we originally chose `peak_performance` as our response variable, but with its noticeably low correlations with most of the predictors, we switched our response to `spotify_track_popularity`

\vspace{10pt}

## Exploration of Spotify Track Popularity on Billboard as Response Variable

`spotify_track_popularity` is correllated with almost all parameters in our data set. This means we are much more likely to be able to actually predict it's popularity based on the values of the other parameters, which will preform better than `peak_performance` did. 

Here we see the two least correlated variables with respect to `spotify_track_popularity`:
```{r no-corr-popularity, warning=FALSE, echo = FALSE, message=FALSE,fig.width = 12, fig.height = 4}
options(repr.plot.width=10, repr.plot.height=4)
mean_popularity <- mean(songs$spotify_track_popularity)
p1 <- ggplot(data = songs, aes(tempo, spotify_track_popularity) ) + 
  geom_point() + geom_smooth(method="lm", aes(color = "Regression Line")) + geom_hline(aes(yintercept = mean_popularity, color = "Mean")) + 
  scale_color_manual(name = "legend", values = c("red", "blue")) +
  labs(x = "Tempo", y = "Spotify Popularity", title = "Spotify Track Popularity Compared to Tempo")

p2 <- ggplot(data = songs, aes(key, spotify_track_popularity) ) + 
  geom_point() + geom_smooth(method="lm", aes(color = "Regression Line")) + geom_hline(aes(yintercept = mean_popularity, color = "Mean")) + 
  scale_color_manual(name = "legend", values = c("red", "blue")) + 
  labs(x = "Key", y = "Spotify Popularity", title = "Spotify Track Popularity Compared to Key")
p1 + p2
```
These two variables have been explained in greater detail in the earlier report, but are still quite relevant to mention as in our data analysis in training and testing models. These two were filtered out due to 0 correlation with the response `spotify_track_popularity` and cut down total number of predictors.

As we've seen, all these variables in the correlation matrix do have more correlation in some form in tandem with `spotify_track_popularity`, hence it should be a better response variable than `peak_performance`, so we will use this as our response variable. 

\vspace{10pt}



### Further Exploration Of Data

```{r, echo = FALSE, fig.width = 12, fig.height = 4}
options(repr.plot.width=10, repr.plot.height=4)
popularity_boxplot = songs %>% 
                      ggplot(aes(x = "", y = spotify_track_popularity)) +
                      geom_boxplot() + 
                      ggtitle("Spotify Track Popularity Boxplot") + 
                      labs(x = "", y = "Spotify Track Popularity") +
                      theme(plot.title = element_text(hjust = 0.5))

pop25 = songs %>% filter(spotify_track_popularity <= 25)
pop50 = songs %>% filter(spotify_track_popularity > 25 & spotify_track_popularity <= 50)
pop75 = songs %>% filter(spotify_track_popularity > 50 & spotify_track_popularity <= 75)
pop100 = songs %>% filter(spotify_track_popularity > 75)

pop25_df = cbind("Pop25", nrow(pop25)) 
pop50_df = cbind("Pop50", nrow(pop50)) 
pop75_df = cbind("Pop75", nrow(pop75)) 
pop100_df = cbind("Pop100", nrow(pop100)) 

pop_all_df = as.data.frame(rbind(pop25_df, pop50_df, pop75_df, pop100_df))
colnames(pop_all_df) = c("Pops", "n")
ordered_pops = c("Pop25", "Pop50", "Pop75", "Pop100")

pop_n_plot = pop_all_df %>%
              ggplot(aes(x = Pops, y = n)) +
              geom_col() +
              scale_x_discrete(limits = ordered_pops) +
              labs(x = "Popularity thresholds", y = "Counts") +
              ggtitle("Counts For Each Popularity Group") +
              theme(plot.title = element_text(hjust = 0.5)) +
              geom_text(aes(label = n), vjust = -0.5, size = 3,
              position = position_dodge(0.9))

popularity_boxplot + pop_n_plot
summary(songs$spotify_track_popularity)
```


Spotify popularity ranges from 0 to 100. The boxplot alongside the five number summary helps us understand that the median popularity is 43. This suggests that a majority of the songs are ranked lower in popularity, hence the higher popularity ranks are coveted.


## Results and Analysis
```{r loading-data-sets, echo=FALSE, fig.height = 4}
train <- read.csv("training.csv")
train <- train %>% dplyr::select(-c("X", "X.1"))
validation <- read.csv("validation.csv")
validation <- validation %>% dplyr::select(-c("X", "X.1"))
test <- read.csv("test.csv")
test <- test %>% dplyr::select(-c("X"))

##Some useful functions for comparison
mse <- function(actualValues, predictedValues){
  mean((actualValues - predictedValues)^2)
}
```

### Training models 

### The pure linear model 

From the exploratory data analysis, we take out the predictor variables with no correlation to the response variable, as well as the other ones explained before in the EDA.

```{r, echo=FALSE}
train_steve <- dplyr::select(train, -performer, -song, -spotify_genre, -spotify_track_album, -date, -key, -tempo)
validation_steve <- dplyr::select(validation, -performer, -song, -spotify_genre, -spotify_track_album, -date, -key, -tempo)
test_steve <- dplyr::select(test, -performer, -song, -spotify_genre, -spotify_track_album, -date, -key, -tempo)
```

Then we train a linear regression model on the training set, and show its diagnostic plots:

```{r, echo = FALSE, fig.align = "center", fig.height=4}
par(mfrow = c(1,2))
full_model <- lm(spotify_track_popularity ~ ., data = train_steve)
plot(full_model, 1)
plot(full_model, 2)
```
The interesting part of the full model is that all predictor variables end up with a very low p-value in the summary of the full model, meaning they all are quite significant in some way when it comes to the response. This is important, as this builds the basis for predictor selection in other reduced models. From our Normal Q-Q plot, it would be reasonable to say the residuals resemble somewhat of a normal distribution, as many points lie near or on the linear line.

As we want to find some optimal combination of predictor variables, we try forward and backwards selection to get some examples. The graphs shown are the BIC and Cp for forward selection and backwards selection of predictors respectively:
```{r, fig.show="hold", out.width="50%", echo = FALSE}
library(leaps)
f_step <- regsubsets(spotify_track_popularity ~ ., data = train_steve, method = "forward")

infocrit_f <- tibble(
  BIC = summary(f_step)$bic, 
  Cp = summary(f_step)$cp,
  size = 1:8)

info_plot_f <- infocrit_f %>% 
  pivot_longer(-size, names_to="crit") %>%
  ggplot(aes(size, value, color=crit)) + 
  geom_point() + 
  geom_line() + 
  facet_wrap(~crit, 2, scales = "free_y") + 
  scale_color_brewer(palette = "Set1") +
  theme(axis.title.y = element_blank(), 
        legend.position = "none")


b_step <- regsubsets(spotify_track_popularity ~ ., data = train_steve, method = "backward")

infocrit_b <- tibble(
  BIC = summary(b_step)$bic, 
  Cp = summary(b_step)$cp,
  size = 1:8)

info_plot_b <- infocrit_b %>% 
  pivot_longer(-size, names_to="crit") %>%
  ggplot(aes(size, value, color=crit)) + 
  geom_point() + 
  geom_line() + 
  facet_wrap(~crit, 2, scales = "free_y") + 
  scale_color_brewer(palette = "Set1") +
  theme(axis.title.y = element_blank(), 
        legend.position = "none")

info_plot_f
info_plot_b
```
From both stepwise selection methods, we can see that BIC and Cp both decrease when having more predictors. This may mean that models with more predictors are much more favored in terms of BIC and Cp, gives us reason to look at modes with more predictor variables, and lines up with our observation of how good our predictors are (small p-value) in our full model.

We first try fitting a model recommended by the selection methods (check code)
```{r, echo=FALSE}
reduced_1 <- lm(spotify_track_popularity ~ loudness + peak_position + spotify_track_explicit + valence + acousticness + danceability + weeks_on_chart + spotify_track_duration_ms, data = train_steve)

```

For fun, we also fit a model based on choosing the variables with a p-value of less than 2e-16 from the full model (check code for predictors):
```{r, echo=FALSE}
reduced_2 <- lm(spotify_track_popularity ~ peak_position + weeks_on_chart + spotify_track_explicit + danceability + loudness + acousticness + valence, data = train_steve)
```
Something noteworthy upon closer inspection is that the reduced model recommended by our stepwise selection methods is very similar to our model based on choosing the predictors with the lowest magnitude of p-values.

Finally, we compare our trained models by using it to predict those in our validation set. We will use MSE as our CV score since we are focused on predictive power:
```{r, echo = FALSE}
mse <- function(preds, obs) mean((obs - preds)^2)

full_mse <- mse(predict(full_model, newdata = validation_steve), test$spotify_track_popularity)

reduced1_mse <- mse(predict(reduced_1, newdata = validation_steve), test$spotify_track_popularity)

best_lm_validation_predictions <- predict(reduced_2, newdata = validation_steve)
reduced2_mse <- mse(best_lm_validation_predictions, test$spotify_track_popularity)

mse_names <- c("full model mse", "reduced model 1 mse", "reduced model 2 mse")
model_mses <- c(full_mse, reduced1_mse, reduced2_mse)

rbind(mse_names, model_mses)
```

From our choices of linear models, we can see that the reduced model of taking all the predictors that had less than 2e-16 for their p-value in the full model performed the best in terms of prediction on our validation set. However, the difference is very marginal. Our best model has an MSE of 689.5128.

Something noteworthy upon closer inspection is that the reduced model recommended by our stepwise selection methods is very similar to our model based on choosing the predictors with the lowest magnitude of p-values.

### Ridge & LASSO Regression Model 
```{r echo = FALSE, eval = FALSE ,error = FALSE}
#import library for lasso regression
library(glmnet)
library(tidyverse)
library(MASS)
set.seed(123)

#import and clean data
train_data <- read.csv("training.csv")
train_data_clean <- dplyr::select(train_data, -X, -performer, -song, -spotify_genre, -spotify_track_album, -date)
x <- as.matrix(train_data_clean[, names(train_data_clean) != "peak_position"])
y <- as.vector(train_data_clean$peak_position)

#50 runs of 5-fold CV
lambdas <- exp(seq(-5, 5, by=0.5))
n <- nrow(x)
k <- 5
ii <- (1:n) %% k + 1
N <- 50
mspe_lasso <- rep(0, N)
mspe_ridge <- rep(0, N)
mspe_full = rep(0, N)
for (i in 1:N) {
  ii <- sample(ii)
  pred_lasso <- rep(0, n)
  pred_ridge <- rep(0, n)
  pred_full = rep(0, n)
  for (j in 1:k) {
    reg_ridge <- cv.glmnet(
      x = x[ii != j, ], y = y[ii != j], lambda = lambdas,
      nfolds = 5, alpha = 0, family = "gaussian"
    )
    reg_lasso <- cv.glmnet(
      x = x[ii != j, ], y = y[ii != j], lambda = lambdas,
      nfolds = 5, alpha = 1, family = "gaussian"
    )
    
    full = lm(peak_position ~ ., data = train_data_clean[ii != j, ])
    pred_ridge[ii == j] <- predict(reg_ridge, s = "lambda.min", newx = x[ii == j, ])
    pred_lasso[ii == j] <- predict(reg_lasso, s = "lambda.min", newx = x[ii == j, ])
    pred_full[ii == j] <- predict(full, newdata = train_data_clean[ii == j, ])
  }
  mspe_ridge[i] <- mean((y - pred_ridge)^2)
  mspe_lasso[i] <- mean((y - pred_lasso)^2)
  mspe_full[i] <- mean((y - pred_full)^2)
}
boxplot(mspe_ridge, mspe_lasso, mspe_full, names = c("Ridge", "Lasso", "Full"))

#plot of regressions
plot(reg_ridge) 
plot(reg_lasso) 
plot(full)
plot(reg_ridge$glmnet.fit, main = 'Ridge')
#abline(v = sum(abs(coef(ridge.glmnet))))
plot(reg_lasso$glmnet.fit, main = 'Lasso')
#abline(v = sum(abs(coef(lasso.glmnet)))) 

# make predictions with regression models
test_data <- read.csv("test.csv")
test_data_clean <- dplyr::select(test_data, -X, -performer, -song, -spotify_genre, -spotify_track_album, -date)
x_test <- as.matrix(test_data_clean[, names(test_data_clean) != "peak_position"])
y_test <- as.vector(test_data_clean$peak_position)

# evaluate model predicitons
ridge_pred <- predict(reg_ridge, newx = x_test, s = "lambda.min")
lasso_pred <- predict(reg_lasso, newx = x_test, s = "lambda.min")
ridge_mse <- mean((y_test - ridge_pred)^2)
lasso_mse <- mean((y_test - lasso_pred)^2)

ridge_mse
lasso_mse
```


### Random Forest Model 
As we've seen in the EDA across the scatterplots, it is not immediately clear that a linear model fits our dataset very well. Since random forests are one of the best "out of the box" models, we will attempt to use random forests to predict the popularity of our songs. We will compare two different models, one with all fields and one with those with 0 correlation to our response removed. 
```{r random-forests, echo=FALSE}
#trains random forest on all fields 
rf_all <- randomForest::randomForest(spotify_track_popularity ~ ., data = train) 
#trains random forest on only correlated parameters 
rf_correlated <- randomForest::randomForest(spotify_track_popularity ~ -key - tempo + ., data = train)
rf_all.predicted <- predict(rf_all, newData = train)
rf_correlated.predicted <- predict(rf_correlated, newData = train)
rf_all.mse <- mse(train$spotify_track_popularity, rf_all.predicted)
rf_correlated.mse <- mse(train$spotify_track_popularity, rf_correlated.predicted)

rf_all.validation.prediction <- predict(rf_all, validation)
rf_all.validation.mse <- mse(validation$spotify_track_popularity, rf_all.validation.prediction)
```

```{r rf-graphs, fig.show="hold", out.width="50%", echo=FALSE}

rf_combined <- cbind(train, rf_all.predicted, rf_correlated.predicted)
ggplot(data = rf_combined, aes(spotify_track_popularity, rf_all.predicted)) + geom_point() +
  geom_abline(slope = 1, intercept = 0, col = 2) + labs(title = "Actual Vs. Predicted Values", subtitle = "All Paramaters Model") + xlab( "Actual") +  ylab ("Predicted")

ggplot(data = rf_combined, aes(spotify_track_popularity, rf_correlated.predicted)) + geom_point() +
  geom_abline(slope = 1, intercept = 0, col = 2) + labs(title = "Actual Vs. Predicted Values", subtitle = "Only Correlated Paramaters Model") + xlab( "Actual") +  ylab ("Predicted")

```

As we can see in the above graphics, where we plotted the expected values versus the predicted values against one another, we get a roughly linear line along the  y = x line shown in red. The models do not differ greatly between one another. We do however see that at the very high/low values our random forest predicts poorer than it does for values in the center. The mean squared error for the model with all parameters is `r rf_all.mse` and for the model removing the uncorrelated parameters is `r rf_correlated.mse`. 

```{r more-rf-plots, fig.show="hold", out.width="50%", echo=FALSE}
varImpPlot(rf_all, main = "Variable Importanct for all Paramater Model")

ggplot(data = rf_combined, aes(date, spotify_track_popularity, colour = "Actual")) + geom_point() +
  geom_point(aes(date, rf_all.predicted, colour = "Predicted")) + labs(title = "Actual Vs. Predicted Values", subtitle = "Only Correlated Paramaters Model")

```

We can see from the above plots that date is by far the biggest contributor to the prediction. This is followed by the peak position on the billboard chard, genre, duration, and weeks on the billboard charts. These all make a lot of sense as the large contributors, the largest surprise to me was date beating billboard peak position, as it would make sense that billboard performance would indicate the Spotify popularity. In the next plot, date, which is clearly not a linear parameter with respect to the popularity is decently well predicted by our random forest. Though as expected from the previous plots, the data is more centered rather than covering the high/low values very well.

### Comparing the models 
``` {r comparing_models, echo = FALSE}
mses_all_models.validation <- tibble(
  `Random Forest` = rf_all.validation.mse, 
  `Linear Regression` = reduced2_mse, 
  #`LASSO` = 
  #`RIDGE`= 
)
kable(mses_all_models.validation, label = "MSE Of All Models on Validation Set")
````
### Putting them all together 
``` {r combining_models, echo = FALSE}
prediction_all_models <- tibble(
  Actual = validation$spotify_track_popularity,
  `Random Forest` = as.numeric(rf_all.validation.prediction), 
  `Linear Regression` = as.numeric(best_lm_validation_predictions), 
  #`LASSO` = 
  #`RIDGE`= 
)
n_models <- ncol(prediction_all_models)
predictions_matrix <- as.matrix(prediction_all_models[2:n_models])


prediction_all_models$MeanPred <- apply(predictions_matrix, 1, mean)
mean_pred_mse <- mse(prediction_all_models$MeanPred, prediction_all_models$Actual)
ensemble_model <- lm(Actual ~ . - MeanPred, data = prediction_all_models )
prediction_all_models$LmPred <- apply(prediction_all_models, 1, function(x) predict(ensemble_model, as.list(x)))
lm_pred_mse <- mse(prediction_all_models$LmPred, prediction_all_models$Actual)
````

### Results and Discussion 

### Results
``` {r test_models, echo = FALSE}
rf.test.pred <- predict(rf_all, test)
lm.test.pred <- predict(reduced_2, test_steve)
#lasso.test.pred <- 
#ridge.test.pred <- 
test_all_models <- tibble(
  Actual = test$spotify_track_popularity,
  `Random Forest` = rf.test.pred, 
  `Linear Regression` = lm.test.pred
  #`LASSO` = 
  #`RIDGE`= 
)
predictions_matrix <- as.matrix(test_all_models[2:n_models])


test_all_models$MeanPred <- apply(predictions_matrix, 1, mean)
test.mean_pred_mse <- mse(test_all_models$MeanPred, test_all_models$Actual)
test_all_models$LmPred <- apply(test_all_models, 1, function(x) predict(ensemble_model, as.list(x)))
test.lm_pred_mse <- mse(test_all_models$LmPred, test_all_models$Actual)
````

```{r summary, echo = FALSE}
test_mses <- double(ncol(test_all_models) - 1)
for(i in 2:ncol(test_all_models)){
  test_mses[i -1] <- mse(test_all_models[,i], test_all_models[,1])
}
kable(test_mses, col.names = c("Random Forest", "Linear Regression", "LASSO", "Ridge"))#, "Mean Ensembling", "Weighted Ensembling"))
```

### Discussion 

### Conclusion

